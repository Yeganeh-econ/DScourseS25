\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{fancyvrb}
% Make sure you have the tabularray and siunitx packages loaded in your preamble
\usepackage{tabularray}
\usepackage{siunitx}
\usepackage{booktabs}

\title{Problem Set 8- Questions 5, 7 and 9}
\author{Yeganeh Karbalaei}
\date{}
\begin{document}
\maketitle

\section{Question 5}
\begin{quote}
\fontfamily{cmss}\selectfont % Change to sans-serif font
 How does your estimate compare with the
true value $\beta$ in (1)?
\end{quote}
\begin{table}[htbp]
\centering
\caption{Comparison of True and Estimated Beta Coefficients}
\begin{tabular}{rrrrr}
\hline
\textbf{Index} & \textbf{True\_Beta} & \textbf{Estimated\_Beta} & \textbf{Difference} \\
\hline
1 & 1.50 & 1.5010518 & 0.0010518190 \\
2 & -1.00 & -1.0008296 & -0.0008296258 \\
3 & -0.25 & -0.2516480 & -0.0016480100 \\
4 & 0.75 & 0.7490406 & -0.0009593796 \\
5 & 3.50 & 3.5005531 & 0.0005531240 \\
6 & -2.00 & -2.0008185 & -0.0008185346 \\
7 & 0.50 & 0.4987148 & -0.0012852344 \\
8 & 1.00 & 1.0028269 & 0.0028268510 \\
9 & 1.25 & 1.2465102 & -0.0034898039 \\
10 & 2.00 & 2.0010012 & 0.0010011821 \\
\hline
\end{tabular}
\label{tab:beta_comparison}
\end{table}
\section{Question 7}
\begin{quote}
\fontfamily{cmss}\selectfont % Change to sans-serif font
Do it again using the Nelder-Mead algorithm. Do your answers differ?
\end{quote}
% Comparison of All Estimation Methods
\begin{table}[htbp]
\centering
\caption{Comparison of Beta Estimates Across Different Methods}
\begin{tabular}{rrrrrr}
\hline
\textbf{Index} & \textbf{True\_Beta} & \textbf{OLS\_Beta} & \textbf{GD\_Beta} & \textbf{LBFGS\_Beta} & \textbf{NM\_Beta} \\
\hline
1 & 1.50 & 1.5010518 & 1.5010515 & 1.5010518 & 1.5010518 \\
2 & -1.00 & -1.0008296 & -1.0008295 & -1.0008296 & -1.0008296 \\
3 & -0.25 & -0.2516480 & -0.2516479 & -0.2516480 & -0.2516480 \\
4 & 0.75 & 0.7490406 & 0.7490405 & 0.7490406 & 0.7490406 \\
5 & 3.50 & 3.5005531 & 3.5005526 & 3.5005531 & 3.5005531 \\
6 & -2.00 & -2.0008185 & -2.0008182 & -2.0008185 & -2.0008185 \\
7 & 0.50 & 0.4987148 & 0.4987147 & 0.4987148 & 0.4987148 \\
8 & 1.00 & 1.0028269 & 1.0028268 & 1.0028269 & 1.0028269 \\
9 & 1.25 & 1.2465102 & 1.2465100 & 1.2465102 & 1.2465102 \\
10 & 2.00 & 2.0010012 & 2.0010009 & 2.0010012 & 2.0010012 \\
\hline
\end{tabular}
\label{tab:methods_comparison}
\end{table}

% discussing the results
\paragraph{Comparison of Estimation Methods}
The table above compares the beta coefficient estimates obtained using different optimization methods: the closed-form OLS solution, gradient descent (GD), L-BFGS algorithm, and Nelder-Mead algorithm. All methods converged to essentially identical solutions, with differences only in the seventh decimal place or beyond. The closed-form OLS solution, L-BFGS, and Nelder-Mead produced identical results to the precision shown. The answers of NM and L-BFGS do not differ. Nelder-Mead Algorithm is a derivative free approach. However, L-BFGS uses gradient information to find the minimum. 

\section{Question 9}
\begin{quote}
\fontfamily{cmss}\selectfont % Change to sans-serif font
In your .tex file, tell me about how similar your estimates of $\hat{\beta}$ are to the "ground truth" $\beta$ that you used to create the data in (1).
\end{quote}

\begin{table}[htbp]
\centering
\caption{Comparison of Estimated Coefficients to Ground Truth}
\begin{tabular}{lrrr}
\hline
\textbf{Parameter} & \textbf{True Value} & \textbf{Estimated Value} & \textbf{Difference} \\
\hline
Intercept & 1.5000 & 1.5011 & 0.0011 \\
X1 & -1.0000 & -1.0008 & -0.0008 \\
X2 & -0.2500 & -0.2516 & -0.0016 \\
X3 & 0.7500 & 0.7490 & -0.0010 \\
X4 & 3.5000 & 3.5006 & 0.0006 \\
X5 & -2.0000 & -2.0008 & -0.0008 \\
X6 & 0.5000 & 0.4987 & -0.0013 \\
X7 & 1.0000 & 1.0028 & 0.0028 \\
X8 & 1.2500 & 1.2465 & -0.0035 \\
X9 & 2.0000 & 2.0010 & 0.0010 \\
\hline
\end{tabular}
\label{tab:coef_comparison}
\end{table}

\paragraph{Comparison of Estimated Coefficients to Ground Truth}
The estimates are very close to the ground truth values, which is expected given the large sample size (N = 100,000). The differences are due to the random error term that was added during data generation. With this large sample size, the Law of Large Numbers ensures that our estimates converge to the true parameter values.

\begin{table}[htbp]
\centering
\caption{OLS Regression Results}
\label{tab:regression}
\begin{tblr}{
  colspec={lc},
  row{1} = {font=\bfseries},
  column{1} = {halign=l},
  column{2} = {halign=c},
  hline{1,Z} = {1-2}{solid},
  hline{2} = {1-2}{solid},
  note{$+$} = {$p < 0.1$},
  note{$*$} = {$p < 0.05$},
  note{$**$} = {$p < 0.01$},
  note{$***$} = {$p < 0.001$},
}
& OLS Model \\
\hline
X1 & 1.501\textsuperscript{***} \\
& (0.002) \\
X2 & -1.001\textsuperscript{***} \\
& (0.002) \\
X3 & -0.252\textsuperscript{***} \\
& (0.002) \\
X4 & 0.749\textsuperscript{***} \\
& (0.002) \\
X5 & 3.501\textsuperscript{***} \\
& (0.002) \\
X6 & -2.001\textsuperscript{***} \\
& (0.002) \\
X7 & 0.499\textsuperscript{***} \\
& (0.002) \\
X8 & 1.003\textsuperscript{***} \\
& (0.002) \\
X9 & 1.247\textsuperscript{***} \\
& (0.002) \\
X10 & 2.001\textsuperscript{***} \\
& (0.002) \\
\hline
R² & 0.991 \\
Adjusted R² & 0.991 \\
Observations & 100000 \\
\end{tblr}
\end{table}

\end{document}